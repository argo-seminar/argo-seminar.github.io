[
  {
    "Speaker": "Matthieu Blanke",
    "Title": "Equations différentielles neuronales",
    "Date": "08/06/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Amaury",
    "Title": "GNN and planted problems",
    "Date": "15/06/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Odilon",
    "Title": "Inférence d'un graphe de contraintes pour la CAO",
    "Date": "22/06/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Mathieu Even",
    "Title": "Optimisation distribuée : asseoir l'idée de partage dans une société de l'individu",
    "Date": "29/06/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Simon Coste",
    "Title": "Rigidité de processus ponctuel : une définition et un théorème",
    "Date": "06/07/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Jean-Guillaume",
    "Title": "(Soutenance de stage)",
    "Date": "21/07/2021",
    "Abstract": ""
  },
  {
    "Speaker": "SUMMER BREAK",
    "Title": "",
    "Date": "",
    "Abstract": ""
  },
  {
    "Speaker": "Eric Daoud",
    "Title": "Caractérisation & accessibilité des centres de soins spécialisés en cancérologie en France métropolitaine",
    "Date": "05/10/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Ludovic Stephan",
    "Title": "(Pré-soutenance de thèse)",
    "Date": "19/10/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Bastien Dubail",
    "Title": "Markovian Linearization of Random Walks on Groups",
    "Date": "26/10/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Edouard Oyallon",
    "Title": "Unsupervised graph representation for node and signal classification",
    "Date": "25/11/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Grégoire Mialon",
    "Title": "GraphiT: Encoding Graph Structure in Transformers",
    "Date": "07/12/2021",
    "Abstract": ""
  },
  {
    "Speaker": "Bruno Loureiro",
    "Title": "Generalization error rates in kernel ridge regression: the crossover from the noiseless to the noisy regime",
    "Date": "07/12/2021",
    "Abstract": ""
  },
  {
    "Speaker": "WINTER BREAK",
    "Title": "",
    "Date": "",
    "Abstract": ""
  },
  {
    "Speaker": "Bertille Follain",
    "Title": "High-dimensional changepoint estimation with heterogeneous missingness",
    "Date": "17/2/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Edwige Cyffers",
    "Title": "Privacy amplification by decentralization",
    "Date": "03/03/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Romain Cosson",
    "Title": "Quantifying Variational Approximation for Log-Partition Function",
    "Date": "04/03/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Cédric Gerbelot",
    "Title": "Statistical physics of learning: a mathematical perspective",
    "Date": "24/03/2022",
    "Abstract": ""
  },
  {
    "Speaker": "David Robin",
    "Title": "Convergence analysis of reparameterized quadratic gradient flow",
    "Date": "31/03/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Christophe Giraud",
    "Title": "The price of unfairness in linear bandits with biased feedback",
    "Date": "14/04/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Matthieu Blanke",
    "Title": "Online greedy identification of linear dynamical systems",
    "Date": "21/04/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Quentin Duchemin",
    "Title": "A new procedure for selective inference with the Generalized Linear Lasso",
    "Date": "19/05/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Marylou Gabrié",
    "Title": "Enhancing Sampling with Learning",
    "Date": "02/06/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Flore Sentenac",
    "Title": "Online Matching in Sparse Random Graphs: Non-Asymptotic Performances of Greedy Algorithm",
    "Date": "09/06/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Maxime Haddouche",
    "Title": "PAC-Bayes learning beyond bounded losses",
    "Date": "21/06/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Ilia Shilov",
    "Title": "Game theoretic approaches for decentralized electricity markets",
    "Date": "23/06/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Virginie Do",
    "Title": "Optimizing generalized Gini indices for fairness in rankings",
    "Date": "30/06/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Simon Coste",
    "Title": "Tutorial on Score-based diffusion models",
    "Date": "07/07/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Luca Ganassalli",
    "Title": "The graph alignment problem: fundamental limits and efficient algorithms",
    "Date": "15/09/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Armand Jordana",
    "Title": "Stagewise Newton Method for Dynamic Game Control with Imperfect State Observation",
    "Date": "22/09/2022",
    "Abstract": ""
  },
  {
    "Speaker": "David Robin",
    "Title": "Convergence beyond the over-parameterized regime with Rayleigh quotients",
    "Date": "10/10/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Théophile Cantelobre",
    "Title": "Measuring dissimilarity with diffeomorphism invariance",
    "Date": "25/10/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Marc Lelarge",
    "Title": "Particle filters for newbies",
    "Date": "07/11/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Batiste Le Bars",
    "Title": "Refined Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data",
    "Date": "14/11/2022",
    "Abstract": ""
  },
  {
    "Speaker": "David Saulpic",
    "Title": "Theoretical Analysis of Community Detection Algorithms",
    "Date": "21/11/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Scott Pesme",
    "Title": "Implicit regularisation of training algorithms in (not so deep) learning",
    "Date": "09/12/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Constantin Philipenko",
    "Title": "Quantization + FL",
    "Date": "14/12/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Jakob Maier",
    "Title": "On Non-Linear operators for Geometric Deep Learning",
    "Date": "10/01/2022",
    "Abstract": ""
  },
  {
    "Speaker": "Alexandre Proutière",
    "Title": "Reinforcement learning with linear representations",
    "Date": "16/01/2023",
    "Abstract": ""
  },
  {
    "Speaker": "Romain Cosson",
    "Title": "Exploration d'arbres en parallèle",
    "Date": "30/01/2023",
    "Abstract": ""
  },
  {
    "Speaker": "Jean Feydy",
    "Title": "Fast geometric libraries for vision and data sciences",
    "Date": "09/02/2023",
    "Abstract": "From 3D point clouds to high-dimensional samples, sparse representations have a key position in the data science toolbox. They complement 2D images and 3D volumes effectively, enabling fast geometric computations for e.g. Gaussian processes and shape analysis. In this talk, I will present extensions for PyTorch, NumPy, Matlab and R that speed up fundamental computations on (generalized) point clouds by several orders of magnitude, when compared to PyTorch or JAX GPU baselines. These software tools allow researchers to break through major computational bottlenecks in the field and have been downloaded more than 400k times over the last few years. "
  },
  {
    "Speaker": "Elisabetta Cornacchia",
    "Title": "Learning with Neural Networks: Generalisation, Complexity and Boolean Measures",
    "Date": "13/02/2023",
    "Abstract": "In this talk, I will consider the learning of logical functions with gradient descent (GD) on neural networks. I will introduce the notion of ``Initial Alignment’’ (INAL) between a neural network at initialization and a target function and show that if a network and target do not have a noticeable INAL, then noisy gradient descent on a fully connected network with i.i.d. initialization cannot learn in polynomial time. Moreover, I will show that on symmetric neural networks, the generalization error can be lower-bounded in terms of the noise-stability of the target function, supporting a conjecture made in prior works. I will further explain how a curriculum learning strategy allows to learn efficiently certain functions (namely dense parities) that are provably hard in the standard setting with no curriculum. If time permits, I will show that in the distribution shift setting, when the data withholding corresponds to freezing a single feature, the generalisation error admits a tight characterisation in terms of the Boolean influence for several relevant architectures. In particular, this puts forward the hypothesis that for such architectures and for learning logical functions, GD tends to have an implicit bias towards low-degree representations."
  },
  {
    "Speaker": "Rémi Flamary",
    "Title": "Optimal transport for graph modeling with applications",
    "Date": "20/02/2023",
    "Abstract": "In recent years the Optimal Transport (OT) based Gromov-Wasserstein (GW) divergence has been investigated as a similarity measure between structured data expressed as distributions typically lying in different metric spaces, such as graphs with arbitrary sizes. In this talk, we will address the optimization problem inherent in the computation of GW and some of its recent extensions, namely the Entropic and the Fused GW divergences. Next we will illustrate how these OT problems can be used to model graph data in learning scenarios such as graph compression, clustering and classification. Finally we will present two recent applications of those OT distances, the first one as a way to align between brains of  fMRI subjects and a second application as a novel pooling layer in Graph Neural Networks"
  },
  {
    "Speaker": "Mathieu",
    "Title": "Implicit regularisation, large stepsizes and edge of stability for (S)GD over diagonal linear networks",
    "Date": "27/02/2023",
    "Abstract": "(description coming)"
  },
  {
    "Speaker": "Vianney Perchet",
    "Title": "Learning learning-augmented algorithms",
    "Date": "22/05/2023",
    "Abstract": "In this talk, I will argue that it is sometimes possible to learn, with techniques originated from bandits, the hints on which learning-augmented algorithms rely to improve worst-case performances. We will describe this phenomenon, the combination of online learning with competitive analysis, on the example of stochastic online scheduling. We shall quantify the merits of this approach by computing and comparing non-asymptotic expected competitive ratios (the standard performance measure of algorithms)."
  },
  {
    "Speaker": "Simon Coste",
    "Title": "Efficient Training of Energy-Based Models Using Jarzinsky Equality",
    "Date": "09/06/2023",
    "Abstract": "Energy-Based models (EBMs) are generative models inspired by statistical physics with a wide range of applications in unsupervised learning. Their performance is best measured by the cross-entropy (CE) of the model distribution relative to the data distribution. Using the CE as the objective for training is however challenging because the computation of its gradient with respect to the model parameters requires sampling the model distribution. Here we show how results for nonequilibrium thermodynamics based on Jarzynski equality together with tools from sequential Monte-Carlo sampling can be used to perform this computation efficiently and avoid the uncontrolled approximations made using the standard contrastive divergence algorithm. Specifically, we introduce a modification of the unadjusted Langevin algorithm (ULA) in which each walker acquires a weight that enables the estimation of the gradient of the cross-entropy at any step during GD, thereby bypassing sampling biases induced by slow mixing of ULA. We illustrate these results with numerical experiments on Gaussian mixture distributions as well1 as the MNIST dataset. We show that the proposed approach outperforms methods based on the contrastive divergence algorithm in all the considered situations."
  },
  {
    "Speaker": "Sean Meyn",
    "Title": "Stochastic Approximation and Machine Learning",
    "Date": "15/06/2023",
    "Abstract": "This lecture aims to explain why stochastic approximation emerges everywhere in our world of algorithms, and why these algorithms typically fail.  Fortunately, there are five rules you can apply to ensure success. An active participant at Inria seminars may recognize that these rules are at odds with what is proposed in recent literature, such as application of non-vanishing step-size for acceleration. Necessary and sufficient conditions are given under which this alternative point of view is valid, but we find that the sufficient conditions rarely hold in applications to reinforcement learning."
  }
]
